# Basics-statistics-for-AI-machine-learning-study
Statistics with Python Specialization [MICHIGAN]( https://coursera.org/share/0c7445ac88c5b05923f937afdd64c925 ) ğŸ”¶ğŸ”· </br>
AI Developer Professional [IBM]( https://coursera.org/share/95fa5c2bf36ea52759dcabc50e1a81b0 ) ğŸ”¶ğŸ”· </br>
Data Analytics [Google]( https://coursera.org/share/3d15025b54bd5680458942a2d4e7c1a6 ) ğŸ”¶ğŸ”· </br>

<p align="center" width="100%">
    <img alt="flappy_distance.jpg" width="34%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/Screenshot%202024-07-14%20010127.png">
    <img alt="flappy_distance.jpg" width="14.2%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/08419ff9-9066-4114-9af4-cca209abc322.jpg"> </br>       <b>Picture from Internet</b></br>
</p>

## Bayesian probability ##
ğŸ§¸ğŸ’¬ In the event there are some events before the current and continue, aligned of all the events created of our as humans nowadays. </br>
ğŸ‘ğŸ’¬ â° They can have multiple events created we need to find the streams and work for each stream as in the application of Speech recognition, NLP, image processing for multiple layers of object detection and augmentation reality, etc. We can have multiple agents queue for communications problems, it is called communications problems because they are updated and comparing results for target results and feedback propagation for the learning process. In multiple agent queues, there are internal updates, communication updates, and experience learning we call scenarios. [Multiple agent queues responses]( https://github.com/jkaewprateep/multi_agent_queue_action_response ) </br>
ğŸ¦­ğŸ’¬ Most processes are individual records meaning multiple records are processed at once by selection but each record is performed for each process with a reference number, there is another approach to aggregate data records that supports multiple records performed from their results. They need reference for logging and verification methods, aggregate tables have specific methods for reference data management such as hash records or rollback compatibility methods. Moreover, some databases need to retain detailed tables with reference to target aggregate data for the verification process too. ( Some processes they need to update results or re-process need to maintain this table data over the retention period ) </br>
ğŸğŸ’¬ Process from designs can be validated and there is an event listener or dataflow diagram as a sample of the Markov model of the data application process because we can work with the state messages in the correct order. Incorrect priority message filters by communication method need to update and verify the event status and result, the state of the message similar to the Markov model as an example. [Markov chain model example]( https://github.com/jkaewprateep/Neuron-Networks-review/blob/main/images/markov-chain.jpg ) </br>
ğŸ‘§ğŸ’¬ ğŸˆ In the event created from an event the between is the process we need to work it done. </br>

<p align="center" width="100%">
    <img alt="flappy_distance.jpg" width="33%" src="https://github.com/jkaewprateep/Neuron-Networks-review/blob/main/images/flappy_distance.jpg">
    <img alt="flappy_distance.jpg" width="40%" src="https://github.com/jkaewprateep/lessonfrom_Applied_Plotting_Charting_and_Data_Representation_in_Python/blob/main/FlappyBird_small.gif"> </br>
    <b>Continuous graph | Sample application</b></br>
</p>

### Neuron networks sample codes ###
ğŸ‘§ğŸ’¬ ğŸˆ The ```super``` because we need to create a new class object, that is an ```initial function``` with new initial weight distribution values because the user often forgets to create a new class and uses the training weight distribution values with another process, one more reason is this class method programming style is short for ```create a new class for work in a process``` and all removed by Python once the reference object did the process or ```none-invoke method```. </br>
ğŸ§¸ğŸ’¬ Some StackOverflow users had questions about the ```multiplication number using Neuron Network```, this is a sample of the custom class and the output target is the shape of the logits output or transform matrix of process values. To make them multiply or work with arithmetic functions you can perform inside the custom class. </br>
ğŸ‘ğŸ’¬ â°  â° â° We want an integration function neuron network ...  ğŸğŸ’¬ There are multiple kinds of neuron networks try find LSTM for different order function calculations. </br>

```
class MyDenseLayer(tf.keras.layers.Layer):
	def __init__(self, num_outputs, name="MyDenseLayer"):
		super(MyDenseLayer, self).__init__()
		self.num_outputs = num_outputs

	def build(self, input_shape):
		self.kernel = self.add_weight("kernel",
		shape=[int(input_shape[-1]), 1])
		self.biases = tf.zeros([int(input_shape[-1]), 1])

	def call(self, inputs):
	
		# Weights from learning effects with input.
		temp = tf.reshape( inputs, shape=(10, 1) )
		temp = tf.matmul( inputs, self.kernel ) + self.biases
		
		# Posibility of x in all and x.
		return tf.nn.softmax( temp, axis=0 )
```

ğŸ§¸ğŸ’¬ **The probability of event $\theta$ created after event $D$ is similar to event $D$ from $\theta$ and the probability of event $\theta$**. Similar to likelihood sequences when the first order in the sequence is $\theta$ and the next is $D$ the probability of $\theta$ and $D$ is less than $\theta$ only and we can manipulate the value with target probabilities to perform some processes such as comparing sequence likelihood, find sources original, create greeting response number from input sequence number and more ...

<p align="center" width="200%">    
    $p(\theta|D) = P(D|\theta)P(\theta)/P(D)$ 
</p>

ğŸ¦­ğŸ’¬ **Sometimes the identical function not always be good in categorizing performance variables**, For example for the Groups category of credit card users the highest values customers are not always the most profitable but steady usage with practical returns on time they are. </br>
ğŸ’ƒ( ğŸ‘©â€ğŸ« )ğŸ’¬ They are the same services in the market VESA and Master card before the transformation about 10 years ago, The VESA process is very good at creating profitability for customers and rewards. For example, aggregation data process, contact time, and communication patterns aggregated data while new data update supportive accumulate in ranges sometimes is **new year event, rewards, or high volumes or weekend**. That is why they need a new year or 6-months synchronized time for accounting calculation time and services maintenance, however, they reduced the time of maintenance by **online aggregation data** they called process online for merchant and online system because they are most time updated since they are merchants using their services. </br>
ğŸ¦¤ğŸ’¬ **Even if it is not necessary they create processes to benefit all parties**, customers, organizations, products, and financial organizations and working processes. </br>

[Neuron-Networks-review]( https://github.com/jkaewprateep/lessonfrom_Applied_Plotting_Charting_and_Data_Representation_in_Python ) </br>
[Applied Plotting]( https://github.com/jkaewprateep/lessonfrom_Applied_Plotting_Charting_and_Data_Representation_in_Python )
</br> 

<p align="center" width="100%">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_01.png">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_03.png"> </br>
    <b>Picture from Internet</b>
</p>

ğŸ§¸ğŸ’¬ Focus on data science, how you identify the sample input data distribution graph example can be an input of coffee and codes return, we need to identify the same group of users tasks from time and inputs with target efficiency â˜•â˜•â˜•. </br>
ğŸ‘ğŸ’¬ â° Log likelihood can perform the process without the need to step each time scale and comparison because of transformation comparison of the green and orange will be in similar value and they are identical. [Likelyhood WiKi]( https://en.wikipedia.org/wiki/Likelihood_function )</br>


<p align="center" width="200%">    
    $O( A1 : A2 | B ) = O( A1 : A2 ) * \Lambda( A1 : A2 | B )$
</p>


### Data generator ###

ğŸ‘§ğŸ’¬ ğŸˆ Create binomial discrete values function ```st.binom.pmf```, sequence numbers ```np.linspace``` and plot into the grid, the continuous graph is an example of ```Bayesian function that is because of consider the event for target relationship or target event prediction```, there are multiple of patterns peaks values create some actions when identical patterns not always the peak values. </br>
ğŸ‘ğŸ’¬ â° ```Series one and Series 6``` not create of high probabilities events and this kind of relationship is not performed by subtraction or ratio values. </br>

```
def lf_plot(n):
    theta = np.linspace(0.01, 0.99, 100)
    for x in np.floor(np.r_[0.2, 0.5, 0.6]*n):
        l = st.binom.pmf(x, n, theta)
        plt.grid(True)
        plt.plot(theta, l, "-", label="%.0f" % x)
        plt.xlabel(r"$\theta$", size=15)
        plt.ylabel("Log likelihood", size=15)
    ha, lb = plt.gca().get_legend_handles_labels()
    plt.figlegend(ha, lb, "center right")
```

ğŸ§¸ğŸ’¬ BETA priors are used for the function characteristics in example $\alpha = \beta = 1$ is center distribution, $\alpha, \beta > 1$ is mode, $\alpha, \beta < 1$  is anti-mode, mean, robustness, concentration and variance are used for technically graphs distribution performance. </br>
ğŸ¦­ğŸ’¬ Confusion matrix, correlation variables, F1-scores, ANOVA, and matrixes are used to find the identical variables, graph determination is also used for experience visualization and it works in the Bayesian series patterns as same as features extraction functions and domain propagation for useful in the process. </br>
ğŸğŸ’¬ Transformation saved time and process by we do not need to perform on exactly every data point or alignment graph, moreover some functions transform and extract significant data patterns such as frequency domain transform or dimension transform. <br>

<p align="center" width="100%">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/lessonfrom_Applied_Plotting_Charting_and_Data_Representation_in_Python/blob/main/02.png">
    <img alt="Statistics distribution" width="32%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_02.png"> </br>
    <b>Continuous graph | Confusion matrix</b></br>
    <b>Picture from Internet</b>
</p>

ğŸ§¸ğŸ’¬ Transforming data information is one of data identical identity, the application of AI machine learning finds patterns that can be used to categorize data, learn, and provide feedback. Marginal chatbox can consider multi-level linear regression problems. In marginal identification internal interceptions/randoms interception is study scopes data response for the function and when x-y interceptions are points of graph where x and y exist then the internal interceptions is a response to one function with existing data values. </br>
ğŸ‘ğŸ’¬ â° â‰ï¸ Is marginal value the patterns â‰ï¸ </br>
ğŸ§¸ğŸ’¬ It is variance and it can be one of the categorized variables, Patterns need to perform similarly in different samples of the inputs and perform the same output as fingerprints or digital ID. </br>

<p align="center" width="100%">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_05.png">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_04.png"> </br>
    <b>Data continuous | Marginal chatbox</b></br>
    <b>Picture from Internet</b>
</p>

## OLS - Ordinary Least Squares ##
ğŸ§¸ğŸ’¬ One method applicable is ordinary least squares, work results, and experiment values easily applied by this method and perform the update with one or two variables concatenation for the robustness of variable input variances problems. </br>   

<p align="center" width="100%">
    <img alt="Statistics distribution" width="30%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_06.png">
    <img alt="Statistics distribution" width="50%" src="https://github.com/jkaewprateep/model_stability/blob/main/Figure_5.png"> </br>
    <b>Continuous BMXBMI | Continuous Flappy bird output variables </b></br>
    <b>Picture from Internet</b>
</p>

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 BPXSY1   R-squared:                       0.215
Model:                            OLS   Adj. R-squared:                  0.214
Method:                 Least Squares   F-statistic:                     697.4
Date:                Wed, 19 Jun 2024   Prob (F-statistic):          1.87e-268
Time:                        06:58:42   Log-Likelihood:                -21505.
No. Observations:                5102   AIC:                         4.302e+04
Df Residuals:                    5099   BIC:                         4.304e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
=====================================================================================
                        coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept           100.6305      0.712    141.257      0.000      99.234     102.027
RIAGENDRx[T.Male]     3.2322      0.459      7.040      0.000       2.332       4.132
RIDAGEYR              0.4739      0.013     36.518      0.000       0.448       0.499
==============================================================================
Omnibus:                      706.732   Durbin-Watson:                   2.036
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1582.730
Skew:                           0.818   Prob(JB):                         0.00
Kurtosis:                       5.184   Cond. No.                         168.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

```

## R-squared and correlation ##
ğŸ§¸ğŸ’¬ In the first step we should create a communication method when working on a project, how much of the selected data variables are related to another or target variable output value? How much they are related to the objective variable values and response function output and how much it effects after applying some value or variables. Variable correlation is one good method to identify first. </br>

**Input:**
```
cc = da[["BPXSY1", "RIDAGEYR"]].corr()
print(cc.BPXSY1.RIDAGEYR**2)
```

**Output:**
```
0.2071545962518702
```

## Residual plot and errors band ##

ğŸ§¸ğŸ’¬ From data observation, this should be done step by data visualization we found that the BMI value matches linear regression with the ordinary least squares model and error bands are in the finite values. </br>   

<p align="center" width="100%">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_07.png">
    <img alt="Statistics distribution" width="40%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_08.png"> </br>
    <b>BMI linear plot | BMI scatter plot </b></br>
    <b>Picture from Internet</b>
</p>

## Logistics regression ##

ğŸ§¸ğŸ’¬ In some application events had persistence scopes $p/(1-p)$ or $p1 = 1 - p2$ . This step is selecting a suitable model for the study and in our example are linear regression model and logistic regression model. </br>

<p align="center" width="100%">
    <img alt="Statistics distribution" width="30%" src="https://github.com/jkaewprateep/Basics-statistics-for-AI-machine-learning-study/blob/main/picture_09.png">
    <img alt="Statistics distribution" width="45%" src="https://github.com/jkaewprateep/counter_clocks/blob/main/98.png"> </br>
    <b>BMI Odd ( logistic regression ) | Sample application in devices </b></br>
    <b>Picture from Internet</b>
</p>

### Sample codes ###
```
temp = tf.random.normal([10], 1, 0.2, tf.float32)
temp = np.asarray(temp) * np.asarray([ coefficient_0, coefficient_1, coefficient_2, coefficient_3, 
coefficient_4, coefficient_5, coefficient_6, coefficient_7, coefficient_8, coefficient_9 ])
action = np.argmax(temp)	
```

### Sample target actions ###
```
actions = { "none_1": K_h, "up_1": K_w, "none_2": K_h, "none_3": K_h, "none_4": K_h, 
                      "none_5": K_h, "none_6": K_h, "none_7": K_h, "none_8": K_h, "none_9": K_h }
```

---

<p align="center" width="100%">
    <img width="30%" src="https://github.com/jkaewprateep/advanced_mysql_topics_notes/blob/main/custom_dataset.png">
    <img width="30%" src="https://github.com/jkaewprateep/advanced_mysql_topics_notes/blob/main/custom_dataset_2.png"> </br>
    <b> ğŸ¥ºğŸ’¬ à¸£à¸±à¸šà¸ˆà¹‰à¸²à¸‡à¹€à¸‚à¸µà¸¢à¸™ functions </b> </br>
</p>
